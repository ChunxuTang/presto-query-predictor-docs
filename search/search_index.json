{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Getting Started presto-query-predictor is a Python module introducing machine learning techniques to the Presto ecosystem. It contains a machine learning pipeline for the model training/evaluation and a query predictor web service to predict CPU and memory usages of Presto queries. Installation After cloning the GitHub repository, pip3 install -e . # Installs the presto-query-predictor package locally pip3 install -r requirements.txt # Installs dependencies An alternative way is to install the package from PyPi, pip3 install presto-query-predictor Note We recommend installing the package in a Python virtual environment instead of installing it globally. Examples The query_predictor/ folder contains the core of the package. We have prepared some examples in the example/ folder, including load_data.py - An example to load the embedded fake TPCH-based dataset. transform.py - An example to transform datasets for further training. train.py - An example to train CPU and memory models. tune.py - An example to tune classification algorithms. app.py - An example to create a query predictor web service. Training A simple way to get a sense of the CPU and memory model training is running the examples in the example/ folder. cd examples python3 transform.py python3 train.py Warning The presto-query-predictor package can only be executed in a Python 3 environment. It does not support Python 2. Afterward, the trained models should be generated in the models folder, including models/ vec-cpu.bin vec-memory.bin model-cpu.bin model-memory.bin By default, the vectorizers are trained from the TF-IDF algorithm, and the models are trained from XGBoost classifiers. The dataset used for training is a faked dataset based on the TPC-H benchmark with only 22 samples. Serving After running python3 app.py A Flask web application should be created at http://0.0.0.0:8000/ . There is a web UI for the application where you can fill in the form with a query for resources prediction.","title":"Getting Started"},{"location":"#getting-started","text":"presto-query-predictor is a Python module introducing machine learning techniques to the Presto ecosystem. It contains a machine learning pipeline for the model training/evaluation and a query predictor web service to predict CPU and memory usages of Presto queries.","title":"Getting Started"},{"location":"#installation","text":"After cloning the GitHub repository, pip3 install -e . # Installs the presto-query-predictor package locally pip3 install -r requirements.txt # Installs dependencies An alternative way is to install the package from PyPi, pip3 install presto-query-predictor Note We recommend installing the package in a Python virtual environment instead of installing it globally.","title":"Installation"},{"location":"#examples","text":"The query_predictor/ folder contains the core of the package. We have prepared some examples in the example/ folder, including load_data.py - An example to load the embedded fake TPCH-based dataset. transform.py - An example to transform datasets for further training. train.py - An example to train CPU and memory models. tune.py - An example to tune classification algorithms. app.py - An example to create a query predictor web service.","title":"Examples"},{"location":"#training","text":"A simple way to get a sense of the CPU and memory model training is running the examples in the example/ folder. cd examples python3 transform.py python3 train.py Warning The presto-query-predictor package can only be executed in a Python 3 environment. It does not support Python 2. Afterward, the trained models should be generated in the models folder, including models/ vec-cpu.bin vec-memory.bin model-cpu.bin model-memory.bin By default, the vectorizers are trained from the TF-IDF algorithm, and the models are trained from XGBoost classifiers. The dataset used for training is a faked dataset based on the TPC-H benchmark with only 22 samples.","title":"Training"},{"location":"#serving","text":"After running python3 app.py A Flask web application should be created at http://0.0.0.0:8000/ . There is a web UI for the application where you can fill in the form with a query for resources prediction.","title":"Serving"},{"location":"analysis/","text":"Log Analysis Usually, we would like to implement an exploratory analysis on the dataset to gain insights before creating an end-to-end machine learning pipeline. The presto-query-predictor contains a Jupyter notebook to achieve that analysis. It contains Basic analysis such as the distribution of principals. Analysis of CPU and memory usages of historical queries. Analysis of Failed queries. To run the notebook, make sure you have installed the Jupyter notebook server locally. Afterward, cd jupyter_notebooks jupyter notebook The notebook server should start up and a browser window should open on your machine, allowing you to choose a notebook from this directory.","title":"Log Analysis"},{"location":"analysis/#log-analysis","text":"Usually, we would like to implement an exploratory analysis on the dataset to gain insights before creating an end-to-end machine learning pipeline. The presto-query-predictor contains a Jupyter notebook to achieve that analysis. It contains Basic analysis such as the distribution of principals. Analysis of CPU and memory usages of historical queries. Analysis of Failed queries. To run the notebook, make sure you have installed the Jupyter notebook server locally. Afterward, cd jupyter_notebooks jupyter notebook The notebook server should start up and a browser window should open on your machine, allowing you to choose a notebook from this directory.","title":"Log Analysis"},{"location":"architecture/","text":"Architecture The high-level design of the package is shown below. In general, the package encapsulates two machine learning models trained from historical Presto queries into a web-based predictor service to predict CPU time and peak memory bytes of future Presto queries. It creates an ML pipeline, including data ingestion, data transformation, vectorization, model training, model evaluation, and model serving. Note As the package employs machine learning techniques, it requires historical request logs for training. And the dataset size may have an impact on the performance of models trained. As we don't need estimation of exact values of CPU time and peak memory bytes, the package applies data discretization to convert the continuous data to discrete data. As a consequence, the package categories the CPU time and peak memory bytes into multiple ranges or buckets. By default, each CPU time falls into one of the four categories: < 30s, 30s - 1h, 1h - 5h, and > 5h ; peak memory bytes fall into one of the three types: < 1MB, 1MB - 1TB, and > 1TB . This approach converts the regression task to a classification problem.","title":"Architecture"},{"location":"architecture/#architecture","text":"The high-level design of the package is shown below. In general, the package encapsulates two machine learning models trained from historical Presto queries into a web-based predictor service to predict CPU time and peak memory bytes of future Presto queries. It creates an ML pipeline, including data ingestion, data transformation, vectorization, model training, model evaluation, and model serving. Note As the package employs machine learning techniques, it requires historical request logs for training. And the dataset size may have an impact on the performance of models trained. As we don't need estimation of exact values of CPU time and peak memory bytes, the package applies data discretization to convert the continuous data to discrete data. As a consequence, the package categories the CPU time and peak memory bytes into multiple ranges or buckets. By default, each CPU time falls into one of the four categories: < 30s, 30s - 1h, 1h - 5h, and > 5h ; peak memory bytes fall into one of the three types: < 1MB, 1MB - 1TB, and > 1TB . This approach converts the regression task to a classification problem.","title":"Architecture"},{"location":"faq/","text":"FAQ How to determine the categories for CPU time and peak memory bytes? It is a critical design decision of categorising the continuous CPU time and peak memory bytes. We believe the specific approach highly depends on the distribution of queries in the dataset and the usages of the models trained. For example, if we would like to apply the models to balance resource-consuming queries across multiple Presto clusters, we may want to put these queries into a specific category. Does the package have real Presto log datasets? No. We don't provide any real Presto log datasets fetched from the production environment. Instead, we created a faked dataset based on TPC-H queries. It has 22 samples in 9 columns. You can also load your datasets for the machine learning pipeline by passing the dataset path to the Pipeline class. See ML Pipeline for more details. Does the package support TensorFlow and/or PyTorch? TensorFlow and PyTorch are both popular deep learning libraries. Currently, we don't support deep learning algorithms, though we have conducted some experiments in RNN/CNN built with TensorFlow. We did see performance improvement with deep learning, but it is not significant. The purpose of this package is to introduce machine learning techniques to the Presto ecosystem, so we want to lower the possible technical barrier as much as possible. But as the design of the pipeline is well-extendable and there is increasing popularity of deep learning, we have a plan to add support of TensorFlow and PyTorch. Does the package support tuning? Hyperparameter tuning is a necessary step to obtain an ideal model. As the purpose of this package is to create an end-to-end ML pipeline for Presto query prediction in the production environment, for now, we only provide limited support for tuning of RandomForestClassifer and LogisticRegressionClassifier . Any contributions are appreciated!","title":"FAQ"},{"location":"faq/#faq","text":"","title":"FAQ"},{"location":"faq/#how-to-determine-the-categories-for-cpu-time-and-peak-memory-bytes","text":"It is a critical design decision of categorising the continuous CPU time and peak memory bytes. We believe the specific approach highly depends on the distribution of queries in the dataset and the usages of the models trained. For example, if we would like to apply the models to balance resource-consuming queries across multiple Presto clusters, we may want to put these queries into a specific category.","title":"How to determine the categories for CPU time and peak memory bytes?"},{"location":"faq/#does-the-package-have-real-presto-log-datasets","text":"No. We don't provide any real Presto log datasets fetched from the production environment. Instead, we created a faked dataset based on TPC-H queries. It has 22 samples in 9 columns. You can also load your datasets for the machine learning pipeline by passing the dataset path to the Pipeline class. See ML Pipeline for more details.","title":"Does the package have real Presto log datasets?"},{"location":"faq/#does-the-package-support-tensorflow-andor-pytorch","text":"TensorFlow and PyTorch are both popular deep learning libraries. Currently, we don't support deep learning algorithms, though we have conducted some experiments in RNN/CNN built with TensorFlow. We did see performance improvement with deep learning, but it is not significant. The purpose of this package is to introduce machine learning techniques to the Presto ecosystem, so we want to lower the possible technical barrier as much as possible. But as the design of the pipeline is well-extendable and there is increasing popularity of deep learning, we have a plan to add support of TensorFlow and PyTorch.","title":"Does the package support TensorFlow and/or PyTorch?"},{"location":"faq/#does-the-package-support-tuning","text":"Hyperparameter tuning is a necessary step to obtain an ideal model. As the purpose of this package is to create an end-to-end ML pipeline for Presto query prediction in the production environment, for now, we only provide limited support for tuning of RandomForestClassifer and LogisticRegressionClassifier . Any contributions are appreciated!","title":"Does the package support tuning?"},{"location":"pipeline/","text":"ML Pipeline Overview A typical machine learning pipeline may contain data loading, data transformation, training(train-dev)/testing datasets splitting, vectorization, classification/ regression, etc. In the presto-query-predictor package, the Pipeline class provides a high-level interface to create a model training pipeline without the necessity to program in each detailed step. The dataset path or dataset, the transformer config path, and the trainer config path are the input parameters for a Pipeline instance. Below is an example to use the class to create a CPU model training pipeline. from pathlib import Path from query_predictor.predictor.pipeline import Pipeline # Path of the embedded fake dataset. # The path can be replaced by other datasets in practical usages. data_path = parent_dir / \"query_predictor/datasets/data/tpch.csv\" # Paths of the default transformer and trainer configs. # The paths should be replaced by other configs in practical usages. transformer_config_path = ( parent_dir / \"query_predictor/conf/transformer.yaml\" ) cpu_trainer_config_path = ( parent_dir / \"query_predictor/conf/trainer-cpu.yaml\" ) # Runs the pipeline to train a model to predict cpu time. cpu_pipeline = Pipeline ( data_path = data_path , transformation_required = True , transformer_config_path = transformer_config_path , trainer_config_path = cpu_trainer_config_path , ) cpu_pipeline . exec () pp = pprint . PrettyPrinter () pp . pprint ( cpu_pipeline . report ) Datasets The package contains a faked dataset created with some TPC-H SQL queries. The faked dataset has 22 samples with columns: query_id, user_, source, environment, catalog, query_state, query, peak_memory_bytes, and cpu_time_ms . The dataset can be loaded through the load_tpch method. from query_predictor.datasets import load_tpch data = load_tpch () print ( data ) Warning The faked dataset is for demo purposes only. You need to train models from some specific Presto request logs for production purposes. Data Transformation After loading a raw Presto request log dataset, we need to transform the dataset, e.g. converting SQL queries to lowercase, creating prediction labels, etc. The package provides a DataTransformer class for data transformation. The required transformations are provided through a transformer configuration file. transformers : # The transformations executed on the dataset - drop_failed_queries # Drops failed queries whose query state is FAILURE - create_labels # Creates prediction labels for CPU time and peak memory bytes - to_lower_queries # Converts SQL queries to lowercase - select_training_columns # Removes unnecessary columns persist : true # Whether the dataset after transformations should be persisted or not persist_path : clean.csv # Persistence path Model Training We apply data vectorization to the query strings in the transformed dataset. For now, based on the scikit-learn vectorizers, the package supports DataCountVectorizer - token count approach DataTfidfVectorizer - TF-IDF (term frequency-inverse document frequency) approach After vectorization, we'll split the dataset to training and testing datasets and apply specific classification algorithms. RandomForestClassifier - A random forest classifier based on the scikit-learn package. LogisticRegressionClassifier - A logistic regression classifier based on the scikit-learn package. XGBoostClassifier - An XGBoost classifier based on the xgboost package. Any contributions to more classifiers are welcome! Both the vectorizer's and the classifier's parameters can be provided through a trainer configuration file. An example of training a CPU model is shown below. label : cpu_time_label # Predictiona label: cpu_time_label or peak_memory_label feature : query # Feature column vectorizer : type : tfidf # Vectorizer type: tfidf or count params : # Params for the vectorizer, following scikit-learn parameters. max_features : 100 min_df : 1 max_df : 0.8 persist : true # Whether the vectorizer trained should be persisted or not persist_path : models/vec-cpu.bin # Persistence path test_size : 0.2 # Testing dataset proportion during splitting classifier : type : XGBoost # Classifier type params : # Params for the classifier max_depth : 2 objective : 'binary:logistic' persist : true # Whether the model trained should be persisted or not persist_path : models/model-cpu.bin # Persistence path After the training, a CPU model should be generated in the models/ folder. This model can be used to predict CPU usages of future Presto requests. Info The vectorizer's and classifier's parameters are for demo purposes. They are not optimized. The parameters usually require tuning when changed to another dataset.","title":"ML Pipeline"},{"location":"pipeline/#ml-pipeline","text":"","title":"ML Pipeline"},{"location":"pipeline/#overview","text":"A typical machine learning pipeline may contain data loading, data transformation, training(train-dev)/testing datasets splitting, vectorization, classification/ regression, etc. In the presto-query-predictor package, the Pipeline class provides a high-level interface to create a model training pipeline without the necessity to program in each detailed step. The dataset path or dataset, the transformer config path, and the trainer config path are the input parameters for a Pipeline instance. Below is an example to use the class to create a CPU model training pipeline. from pathlib import Path from query_predictor.predictor.pipeline import Pipeline # Path of the embedded fake dataset. # The path can be replaced by other datasets in practical usages. data_path = parent_dir / \"query_predictor/datasets/data/tpch.csv\" # Paths of the default transformer and trainer configs. # The paths should be replaced by other configs in practical usages. transformer_config_path = ( parent_dir / \"query_predictor/conf/transformer.yaml\" ) cpu_trainer_config_path = ( parent_dir / \"query_predictor/conf/trainer-cpu.yaml\" ) # Runs the pipeline to train a model to predict cpu time. cpu_pipeline = Pipeline ( data_path = data_path , transformation_required = True , transformer_config_path = transformer_config_path , trainer_config_path = cpu_trainer_config_path , ) cpu_pipeline . exec () pp = pprint . PrettyPrinter () pp . pprint ( cpu_pipeline . report )","title":"Overview"},{"location":"pipeline/#datasets","text":"The package contains a faked dataset created with some TPC-H SQL queries. The faked dataset has 22 samples with columns: query_id, user_, source, environment, catalog, query_state, query, peak_memory_bytes, and cpu_time_ms . The dataset can be loaded through the load_tpch method. from query_predictor.datasets import load_tpch data = load_tpch () print ( data ) Warning The faked dataset is for demo purposes only. You need to train models from some specific Presto request logs for production purposes.","title":"Datasets"},{"location":"pipeline/#data-transformation","text":"After loading a raw Presto request log dataset, we need to transform the dataset, e.g. converting SQL queries to lowercase, creating prediction labels, etc. The package provides a DataTransformer class for data transformation. The required transformations are provided through a transformer configuration file. transformers : # The transformations executed on the dataset - drop_failed_queries # Drops failed queries whose query state is FAILURE - create_labels # Creates prediction labels for CPU time and peak memory bytes - to_lower_queries # Converts SQL queries to lowercase - select_training_columns # Removes unnecessary columns persist : true # Whether the dataset after transformations should be persisted or not persist_path : clean.csv # Persistence path","title":"Data Transformation"},{"location":"pipeline/#model-training","text":"We apply data vectorization to the query strings in the transformed dataset. For now, based on the scikit-learn vectorizers, the package supports DataCountVectorizer - token count approach DataTfidfVectorizer - TF-IDF (term frequency-inverse document frequency) approach After vectorization, we'll split the dataset to training and testing datasets and apply specific classification algorithms. RandomForestClassifier - A random forest classifier based on the scikit-learn package. LogisticRegressionClassifier - A logistic regression classifier based on the scikit-learn package. XGBoostClassifier - An XGBoost classifier based on the xgboost package. Any contributions to more classifiers are welcome! Both the vectorizer's and the classifier's parameters can be provided through a trainer configuration file. An example of training a CPU model is shown below. label : cpu_time_label # Predictiona label: cpu_time_label or peak_memory_label feature : query # Feature column vectorizer : type : tfidf # Vectorizer type: tfidf or count params : # Params for the vectorizer, following scikit-learn parameters. max_features : 100 min_df : 1 max_df : 0.8 persist : true # Whether the vectorizer trained should be persisted or not persist_path : models/vec-cpu.bin # Persistence path test_size : 0.2 # Testing dataset proportion during splitting classifier : type : XGBoost # Classifier type params : # Params for the classifier max_depth : 2 objective : 'binary:logistic' persist : true # Whether the model trained should be persisted or not persist_path : models/model-cpu.bin # Persistence path After the training, a CPU model should be generated in the models/ folder. This model can be used to predict CPU usages of future Presto requests. Info The vectorizer's and classifier's parameters are for demo purposes. They are not optimized. The parameters usually require tuning when changed to another dataset.","title":"Model Training"},{"location":"serving/","text":"Model Serving The presto-query-predictor package implemented a Flask web application for model serving. The service is encapsulated in the predictor_app variable, which can be easily used by running from query_predictor.predictor.predictor_app import predictor_app predictor_app . run () There are two API endpoints: /v1/cpu This API endpoint receives an HTTP request with the query statement carried in the query field as a JSON message in the request body. It returns a response with the expected CPU time range wrapped in the response body. An example of the response body is shown below. { \"cpu_pred_label\" : 0 , \"cpu_pred_str\" : \"< 30s\" } /v1/memory This API endpoint receives an HTTP request with the query statement carried in the query field as a JSON message in the request body. It returns a response with the expected peak memory bytes range wrapped in the response body. An example of the response body is shown below. { \"memory_pred_label\" : 0 , \"memory_pred_str\" : \"< 1MB\" } The web service requires four models trained beforehand: CPU vectorization model Memory vectorization model CPU classification model Memory classification model The parameters about these models can be provisioned through a serving configuration YAML file. An example is shown below. models : cpu_model : label : cpu_time_label feature : query type : XGBoost path : models/model-cpu.bin name : XGBoost-CPU description : An XGBoost model to predict cpu time of each SQL query version : 0.1.0 memory_model : label : peak_memory_label feature : query type : XGBoost path : models/model-memory.bin name : XGBoost-Memory description : An XGBoost model to predict peak memory bytes of each SQL query version : 0.1.0 vectorizers : cpu_vectorizer : feature : query type : tfidf path : models/vec-cpu.bin name : tfidf-cpu description : A TF-IDF vectorizer for SQL queries version : 0.1.0 memory_vectorizer : feature : query type : tfidf path : models/vec-memory.bin name : tfidf-memory description : A TF-IDF vectorizer for SQL queries version : 0.1.0 Info The predictor_app provides a simple interface to serve the models in the production environment. You can also use other web frameworks to serve these models.","title":"Model Serving"},{"location":"serving/#model-serving","text":"The presto-query-predictor package implemented a Flask web application for model serving. The service is encapsulated in the predictor_app variable, which can be easily used by running from query_predictor.predictor.predictor_app import predictor_app predictor_app . run () There are two API endpoints: /v1/cpu This API endpoint receives an HTTP request with the query statement carried in the query field as a JSON message in the request body. It returns a response with the expected CPU time range wrapped in the response body. An example of the response body is shown below. { \"cpu_pred_label\" : 0 , \"cpu_pred_str\" : \"< 30s\" } /v1/memory This API endpoint receives an HTTP request with the query statement carried in the query field as a JSON message in the request body. It returns a response with the expected peak memory bytes range wrapped in the response body. An example of the response body is shown below. { \"memory_pred_label\" : 0 , \"memory_pred_str\" : \"< 1MB\" } The web service requires four models trained beforehand: CPU vectorization model Memory vectorization model CPU classification model Memory classification model The parameters about these models can be provisioned through a serving configuration YAML file. An example is shown below. models : cpu_model : label : cpu_time_label feature : query type : XGBoost path : models/model-cpu.bin name : XGBoost-CPU description : An XGBoost model to predict cpu time of each SQL query version : 0.1.0 memory_model : label : peak_memory_label feature : query type : XGBoost path : models/model-memory.bin name : XGBoost-Memory description : An XGBoost model to predict peak memory bytes of each SQL query version : 0.1.0 vectorizers : cpu_vectorizer : feature : query type : tfidf path : models/vec-cpu.bin name : tfidf-cpu description : A TF-IDF vectorizer for SQL queries version : 0.1.0 memory_vectorizer : feature : query type : tfidf path : models/vec-memory.bin name : tfidf-memory description : A TF-IDF vectorizer for SQL queries version : 0.1.0 Info The predictor_app provides a simple interface to serve the models in the production environment. You can also use other web frameworks to serve these models.","title":"Model Serving"}]}